{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "K = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/query.csv'\n",
    "file = open(filename,'r')\n",
    "text = re.sub('[0-9]+', '', file.read())\n",
    "file.close()\n",
    "queries = text.split('\\n')\n",
    "#queries = pd.read_csv(filename)\n",
    "#queries.columns = ['No.','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the common words to be used as stop words\n",
    "filename = 'cacm/common_words'\n",
    "file = open(filename,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "common_words = text.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createFileForArticle(article,count):\n",
    "    file_name = 'cacm/cacm_seperated/cacm_' + str(count)\n",
    "    lines = article.split('\\n')\n",
    "    new_file = open(file_name,'w+')\n",
    "    filenames.append(file_name)\n",
    "    for line in lines:     \n",
    "       \n",
    "        if(line.startswith('.X')):\n",
    "            break\n",
    "        if(not(line.startswith(('.T','.B','.A','.N','.X','.I')))):\n",
    "            line_cleaned = re.sub('[0-9]+', '', line)\n",
    "            line_cleaned = line_cleaned.lower()\n",
    "            new_file.write(line_cleaned)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_cacm = 'cacm/cacm.all'\n",
    "file = open(filename_cacm,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "count = 0\n",
    "articles = text.split('.I ')\n",
    "for article in articles:\n",
    "    count+=1\n",
    "    createFileForArticle(article,count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve all files in cacm_seperated folder\n",
    "import re\n",
    "corpus=[]\n",
    "\n",
    "for entry in filenames:\n",
    "    file = open(str(entry),'rt')\n",
    "    text = re.sub('[0-9]+', '', file.read())\n",
    "    corpus.append(text.lower())\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords       = open ('cacm/common_words', 'rt').read()\n",
    "stopWordList    = stopWords.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in tokenizer.tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            \n",
    "            #remove tags\n",
    "            token=re.sub(\"\",\"\",token)\n",
    "    \n",
    "            # remove special characters and digits\n",
    "            token=re.sub(\"(\\\\d|\\\\W)+\",\" \",token)\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZETTA\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',tokenizer=tokenize_and_stem,analyzer='word',max_df=0.9)\n",
    "docs_tfidf = vectorizer.fit_transform(corpus)\n",
    "filename = 'cacm/results/result.txt'\n",
    "file = open(filename,'w+')\n",
    "\n",
    "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query,query_no):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(cosineSimilarities)), reverse=True)\n",
    "    for t in sorted_similarity[:K]:\n",
    "        #print(t[0])\n",
    "        file.write(str(query_no) + \" \" +str(t[1]) + \"\\n\")\n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for index,query in enumerate(queries):\n",
    "    similarity = get_tf_idf_query_similarity(vectorizer,docs_tfidf,query,index+1)\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(similarity)), reverse=True)\n",
    "    #print(index+1,(sorted(similarity,reverse=True)[:3])\n",
    "    #print(index+1,similarity)\n",
    "    #results.append(index+1)\n",
    "    #results.append(sorted_similarity[:1])\n",
    "    #results.append((-similarity).argsort()[:5]+1)\n",
    "    #results.append(sorted(similarity,reverse=True)[:3])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to measure accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/qrels.text'\n",
    "qrels = pd.read_csv(filename,sep=\"\\s+\")\n",
    "qrels.columns = ['query', 'document','score','relevance']\n",
    "qrels.drop(columns=['score','relevance'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/results/result.txt'\n",
    "preds = pd.read_csv(filename,sep=\"\\s+\")\n",
    "preds.columns = ['query', 'document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(query_id):\n",
    "    no_of_relevant_documents = qrels['query'].loc[qrels['query'] == query_id].count() or 1 \n",
    "    no_of_documents_retrieved= K\n",
    "    X = (preds['document'].loc[preds['query'] == query_id])\n",
    "    y = (qrels['document'].loc[qrels['query'] == query_id])\n",
    "    X_y_merged = pd.merge(X, y, how ='inner', on =['document'])\n",
    "    no_of_relevant_documents_retrieved = X_y_merged['document'].count()\n",
    "    #no_of_relevant_documents_retrieved = (preds.loc[preds['query'] == query_id])\n",
    "    \n",
    "    precision = no_of_relevant_documents_retrieved / K\n",
    "    recall = no_of_relevant_documents_retrieved / no_of_relevant_documents\n",
    "    return precision,recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.96 6.0\n",
      "2 1.0 8.333333333333334\n",
      "3 1.0 4.166666666666667\n",
      "4 1.0 2.0833333333333335\n",
      "5 1.0 3.125\n",
      "6 1.0 8.333333333333334\n",
      "7 1.0 0.8928571428571429\n",
      "8 1.0 8.333333333333334\n",
      "9 1.0 2.7777777777777777\n",
      "10 1.0 0.7142857142857143\n",
      "11 1.0 1.3157894736842106\n",
      "12 1.0 5.0\n",
      "13 1.0 2.272727272727273\n",
      "14 1.0 0.5681818181818182\n",
      "15 1.0 2.5\n",
      "16 1.0 1.4705882352941178\n",
      "17 1.0 1.5625\n",
      "18 1.0 2.272727272727273\n",
      "19 1.0 2.272727272727273\n",
      "20 1.0 8.333333333333334\n",
      "21 1.0 2.272727272727273\n",
      "22 1.0 1.4705882352941178\n",
      "23 1.0 6.25\n",
      "24 1.0 1.9230769230769231\n",
      "25 1.0 0.49019607843137253\n",
      "26 1.0 0.8333333333333334\n",
      "27 1.0 0.8620689655172413\n",
      "28 1.0 5.0\n",
      "29 1.0 1.3157894736842106\n",
      "30 1.0 6.25\n",
      "31 1.0 12.5\n",
      "32 1.0 8.333333333333334\n",
      "33 1.0 25.0\n",
      "34 1.0 25.0\n",
      "35 1.0 25.0\n",
      "36 1.0 1.25\n",
      "37 1.0 2.0833333333333335\n",
      "38 1.0 1.5625\n",
      "39 1.0 2.0833333333333335\n",
      "40 1.0 2.5\n",
      "41 1.0 25.0\n",
      "42 1.0 1.1904761904761905\n",
      "43 1.0 0.6097560975609756\n",
      "44 1.0 1.4705882352941178\n",
      "45 1.0 0.9615384615384616\n",
      "46 1.0 25.0\n",
      "47 1.0 25.0\n",
      "48 1.0 2.0833333333333335\n",
      "49 1.0 3.125\n",
      "50 1.0 25.0\n",
      "51 1.0 25.0\n",
      "52 1.0 25.0\n",
      "53 1.0 25.0\n",
      "54 1.0 25.0\n",
      "55 1.0 25.0\n",
      "56 1.0 25.0\n",
      "57 1.0 25.0\n",
      "58 1.0 0.8333333333333334\n",
      "59 1.0 0.5813953488372093\n",
      "60 1.0 0.9259259259259259\n",
      "61 1.0 0.8064516129032258\n",
      "62 1.0 3.125\n",
      "63 1.0 2.0833333333333335\n",
      "64 1.0 25.0\n",
      "0.999375 8.157889179190569\n"
     ]
    }
   ],
   "source": [
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "for query_id in range(1,len(queries)):\n",
    "    p,r = get_precision(query_id)\n",
    "    print(query_id,p,r)\n",
    "    sum_precision+=p\n",
    "    sum_recall+=r\n",
    "print(sum_precision/52,sum_recall/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
