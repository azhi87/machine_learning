{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "K = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/query.csv'\n",
    "file = open(filename,'r')\n",
    "text = re.sub('[0-9]+', '', file.read())\n",
    "file.close()\n",
    "queries = text.split('\\n')\n",
    "\n",
    "#queries = pd.read_csv(filename)\n",
    "#queries.columns = ['No.','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the common words to be used as stop words\n",
    "filename = 'cacm/common_words'\n",
    "file = open(filename,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "common_words = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createFileForArticle(article,count):\n",
    "    file_name = 'cacm/cacm_seperated/cacm_' + str(count)\n",
    "    lines = article.split('\\n')\n",
    "    new_file = open(file_name,'w+')\n",
    "    filenames.append(file_name)\n",
    "    for line in lines:     \n",
    "       \n",
    "        if(line.startswith('.X')):\n",
    "            break\n",
    "        if(not(line.startswith(('.T','.B','.A','.N','.X','.I')))):   \n",
    "            new_file.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_cacm = 'cacm/cacm.all'\n",
    "file = open(filename_cacm,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "count = 0\n",
    "articles = text.split('.I ')\n",
    "for article in articles:\n",
    "    count+=1\n",
    "    createFileForArticle(article,count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve all files in cacm_seperated folder\n",
    "import re\n",
    "corpus=[]\n",
    "\n",
    "for entry in filenames:\n",
    "    file = open(str(entry),'rt')\n",
    "    text = re.sub('[0-9]+', '', file.read())\n",
    "    corpus.append(text)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords       = open ('cacm/common_words', 'rt').read()\n",
    "stopWordList    = stopWords.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-18652ba94024>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize_and_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize_and_stem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdocs_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'common_words' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=tokenize_and_stem(str(common_words)),tokenizer=tokenize_and_stem)\n",
    "docs_tfidf = vectorizer.fit_transform(corpus)\n",
    "filename = 'cacm/results/result.txt'\n",
    "file = open(filename,'w+')\n",
    "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query,query_no):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(cosineSimilarities)), reverse=True)\n",
    "    for t in sorted_similarity[:K]:\n",
    "        file.write(str(query_no) + \" \" +str(t[1]) + \"\\n\")\n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for index,query in enumerate(queries):\n",
    "    similarity = get_tf_idf_query_similarity(vectorizer,docs_tfidf,query,index+1)\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(similarity)), reverse=True)\n",
    "    #print(index+1,(sorted(similarity,reverse=True)[:3])\n",
    "    #print(index+1,similarity)\n",
    "    results.append(index+1)\n",
    "    results.append(sorted_similarity[:1])\n",
    "    #results.append((-similarity).argsort()[:5]+1)\n",
    "    #results.append(sorted(similarity,reverse=True)[:3])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/\n",
    "#https://stackoverflow.com/questions/57124182/how-to-fix-stopwords-preprocessing-inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to measure accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/qrels.text'\n",
    "qrels = pd.read_csv(filename,sep=\"\\s+\")\n",
    "qrels.columns = ['query', 'document','score','relevance']\n",
    "qrels.drop(columns=['score','relevance'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/results/result.txt'\n",
    "preds = pd.read_csv(filename,sep=\"\\s+\")\n",
    "preds.columns = ['query', 'document']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(query_id):\n",
    "    no_of_relevant_documents = qrels['query'].loc[qrels['query'] == query_id].count() or 1\n",
    "    no_of_documents_retrieved= K\n",
    "    X = (preds['document'].loc[preds['query'] == query_id])\n",
    "    y = (qrels['document'].loc[qrels['query'] == query_id])\n",
    "    X_y_merged = pd.merge(X, y, how ='inner', on =['document'])\n",
    "    no_of_relevant_documents_retrieved = X_y_merged['document'].count()\n",
    "    #no_of_relevant_documents_retrieved = (preds.loc[preds['query'] == query_id])\n",
    "    \n",
    "    precision = no_of_relevant_documents_retrieved / K\n",
    "    recall = no_of_relevant_documents_retrieved / no_of_relevant_documents\n",
    "    return precision,recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15572916666666667 0.3788354540708639\n"
     ]
    }
   ],
   "source": [
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "for query_id in range(1,len(queries)):\n",
    "    p,r = get_precision(query_id)\n",
    "    sum_precision+=p\n",
    "    sum_recall+=r\n",
    "print(sum_precision/64,sum_recall/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
