{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "K = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/query.csv'\n",
    "file = open(filename,'r')\n",
    "text = re.sub('[0-9]+', '', file.read())\n",
    "file.close()\n",
    "queries = text.split('\\n')\n",
    "#queries = pd.read_csv(filename)\n",
    "#queries.columns = ['No.','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the common words to be used as stop words\n",
    "filename = 'cacm/common_words'\n",
    "file = open(filename,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "common_words = text.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createFileForArticle(article,count):\n",
    "    file_name = 'cacm/cacm_seperated/cacm_' + str(count)\n",
    "    lines = article.split('\\n')\n",
    "    new_file = open(file_name,'w+')\n",
    "    filenames.append(file_name)\n",
    "    for line in lines:     \n",
    "       \n",
    "        if(line.startswith('.X')):\n",
    "            break\n",
    "        if(not(line.startswith(('.T','.B','.A','.N','.X','.I')))):\n",
    "            line_cleaned = re.sub('[0-9]+', '', line)\n",
    "            line_cleaned = line_cleaned.lower()\n",
    "            new_file.write(line_cleaned)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_cacm = 'cacm/cacm.all'\n",
    "file = open(filename_cacm,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "count = 0\n",
    "articles = text.split('.I ')\n",
    "for article in articles:\n",
    "    count+=1\n",
    "    createFileForArticle(article,count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve all files in cacm_seperated folder\n",
    "import re\n",
    "corpus=[]\n",
    "\n",
    "for entry in filenames:\n",
    "    file = open(str(entry),'rt')\n",
    "    text = re.sub('[0-9]+', '', file.read())\n",
    "    corpus.append(text.lower())\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords       = open ('cacm/common_words', 'rt').read()\n",
    "stopWordList    = stopWords.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in tokenizer.tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            \n",
    "            #remove tags\n",
    "            token=re.sub(\"\",\"\",token)\n",
    "    \n",
    "            # remove special characters and digits\n",
    "            token=re.sub(\"(\\\\d|\\\\W)+\",\" \",token)\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZETTA\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',tokenizer=tokenize_and_stem,analyzer='word',max_df=0.9)\n",
    "docs_tfidf = vectorizer.fit_transform(corpus)\n",
    "filename = 'cacm/results/result.txt'\n",
    "file = open(filename,'w+')\n",
    "\n",
    "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query,query_no):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(cosineSimilarities)), reverse=True)\n",
    "    for t in sorted_similarity[:K]:\n",
    "        #print(t[0])\n",
    "        file.write(str(query_no) + \" \" +str(t[1]) + \"\\n\")\n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for index,query in enumerate(queries):\n",
    "    similarity = get_tf_idf_query_similarity(vectorizer,docs_tfidf,query,index+1)\n",
    "    sorted_similarity = sorted(((v, i+1) for i, v in enumerate(similarity)), reverse=True)\n",
    "    #print(index+1,(sorted(similarity,reverse=True)[:3])\n",
    "    #print(index+1,similarity)\n",
    "    #results.append(index+1)\n",
    "    #results.append(sorted_similarity[:1])\n",
    "    #results.append((-similarity).argsort()[:5]+1)\n",
    "    #results.append(sorted(similarity,reverse=True)[:3])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to measure accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/qrels.text'\n",
    "qrels = pd.read_csv(filename,sep=\"\\s+\")\n",
    "qrels.columns = ['query', 'document','score','relevance']\n",
    "qrels.drop(columns=['score','relevance'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the query relevance results and store it in a pandas datafrom\n",
    "filename = 'cacm/results/result.txt'\n",
    "preds = pd.read_csv(filename,sep=\"\\s+\")\n",
    "preds.columns = ['query', 'document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(query_id):\n",
    "    no_of_relevant_documents = qrels['query'].loc[qrels['query'] == query_id].count() or 1 \n",
    "    no_of_documents_retrieved= K\n",
    "    X = (preds['document'].loc[preds['query'] == query_id])\n",
    "    y = (qrels['document'].loc[qrels['query'] == query_id])\n",
    "    X_y_merged = pd.merge(X, y, how ='inner', on =['document'])\n",
    "    no_of_relevant_documents_retrieved = X_y_merged['document'].count()\n",
    "    #no_of_relevant_documents_retrieved = (preds.loc[preds['query'] == query_id])\n",
    "    \n",
    "    precision = no_of_relevant_documents_retrieved / K\n",
    "    recall = no_of_relevant_documents_retrieved / no_of_relevant_documents\n",
    "    return precision,recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.08 0.5\n",
      "2 0.12 1.0\n",
      "3 0.04 0.16666666666666666\n",
      "4 0.12 0.25\n",
      "5 0.08 0.25\n",
      "6 0.12 1.0\n",
      "7 0.36 0.32142857142857145\n",
      "8 0.04 0.3333333333333333\n",
      "9 0.2 0.5555555555555556\n",
      "10 0.6 0.42857142857142855\n",
      "11 0.4 0.5263157894736842\n",
      "12 0.08 0.4\n",
      "13 0.2 0.45454545454545453\n",
      "14 0.64 0.36363636363636365\n",
      "15 0.12 0.3\n",
      "16 0.2 0.29411764705882354\n",
      "17 0.2 0.3125\n",
      "18 0.12 0.2727272727272727\n",
      "19 0.32 0.7272727272727273\n",
      "20 0.04 0.3333333333333333\n",
      "21 0.04 0.09090909090909091\n",
      "22 0.48 0.7058823529411765\n",
      "23 0.04 0.25\n",
      "24 0.08 0.15384615384615385\n",
      "25 0.6 0.29411764705882354\n",
      "26 0.36 0.3\n",
      "27 0.48 0.41379310344827586\n",
      "28 0.16 0.8\n",
      "29 0.6 0.7894736842105263\n",
      "30 0.08 0.5\n",
      "31 0.08 1.0\n",
      "32 0.08 0.6666666666666666\n",
      "33 0.04 1.0\n",
      "34 0.0 0.0\n",
      "35 0.0 0.0\n",
      "36 0.28 0.35\n",
      "37 0.16 0.3333333333333333\n",
      "38 0.4 0.625\n",
      "39 0.32 0.6666666666666666\n",
      "40 0.24 0.6\n",
      "41 0.0 0.0\n",
      "42 0.16 0.19047619047619047\n",
      "43 0.32 0.1951219512195122\n",
      "44 0.12 0.17647058823529413\n",
      "45 0.44 0.4230769230769231\n",
      "46 0.0 0.0\n",
      "47 0.0 0.0\n",
      "48 0.04 0.08333333333333333\n",
      "49 0.16 0.5\n",
      "50 0.0 0.0\n",
      "51 0.0 0.0\n",
      "52 0.0 0.0\n",
      "53 0.0 0.0\n",
      "54 0.0 0.0\n",
      "55 0.0 0.0\n",
      "56 0.0 0.0\n",
      "57 0.04 1.0\n",
      "58 0.4 0.3333333333333333\n",
      "59 0.56 0.32558139534883723\n",
      "60 0.4 0.37037037037037035\n",
      "61 0.56 0.45161290322580644\n",
      "62 0.16 0.5\n",
      "63 0.4 0.8333333333333334\n",
      "64 0.04 1.0\n",
      "0.2384615384615385 0.4752385223968627\n"
     ]
    }
   ],
   "source": [
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "for query_id in range(1,len(queries)):\n",
    "    p,r = get_precision(query_id)\n",
    "    print(query_id,p,r)\n",
    "    sum_precision+=p\n",
    "    sum_recall+=r\n",
    "print(sum_precision/52,sum_recall/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
